# Web Scraping e An√°lise de Dados das √öltimas Postagens da Scientific American

**Aluna:** Emily Sato
**RA:** 770567

## üìö Descri√ß√£o do Projeto

Este projeto tem como objetivo **extrair** e **analisar** dados das diversas √∫ltimas postagens do site [Scientific American](https://www.scientificamerican.com/). Utilizando t√©cnicas avan√ßadas de web scraping, conseguimos coletar informa√ß√µes detalhadas sobre os artigos publicados recentemente, armazenando-os em formato JSON na pasta `data`. Posteriormente, realizamos uma an√°lise aprofundada desses dados em um notebook Jupyter, gerando visualiza√ß√µes que destacam tend√™ncias, caracter√≠sticas e insights sobre o conte√∫do divulgado pelo site.

## üöÄ Tecnologias e Ferramentas Utilizadas

- **Linguagem de Programa√ß√£o**: Python
- **Bibliotecas de Web Scraping**:
  - `requests`: Para realizar requisi√ß√µes HTTP e obter o conte√∫do das p√°ginas.
  - `BeautifulSoup` (`bs4`): Para parsear o HTML e extrair as informa√ß√µes desejadas.
- **An√°lise de Dados**:
  - `pandas`: Para manipula√ß√£o e an√°lise de dados estruturados.
  - `numpy`: Para opera√ß√µes num√©ricas e manipula√ß√£o de arrays.
- **Visualiza√ß√£o de Dados**:
  - `seaborn`: Para cria√ß√£o de gr√°ficos estat√≠sticos atraentes.
  - `matplotlib`: Para visualiza√ß√µes b√°sicas e personalizadas.
  - `wordcloud`: Para gerar nuvens de palavras a partir dos resumos dos artigos.
- **Processamento de Texto**:
  - `re` (Express√µes Regulares): Para limpeza e formata√ß√£o dos textos extra√≠dos.
- **Modelagem de T√≥picos**:
  - `scikit-learn` (`CountVectorizer`, `LatentDirichletAllocation`): Para realizar an√°lise de t√≥picos (LDA) nos textos completos dos artigos.
- **Ambiente de Desenvolvimento**:
  - Google Colab: Para integrar c√≥digo, visualiza√ß√µes e an√°lises em um √∫nico documento interativo.

## üõ†Ô∏è Metodologia

### 1. **Escolha do Site e Identifica√ß√£o da Estrutura HTML**
Selecionamos o site [Scientific American](https://www.scientificamerican.com/) devido √† sua relev√¢ncia e credibilidade hist√≥rica na divulga√ß√£o cient√≠fica. A primeira etapa envolveu analisar a estrutura HTML das p√°ginas de √∫ltimas postagens para identificar os elementos que cont√™m as informa√ß√µes desejadas, como t√≠tulo, link, categoria, data, resumo e autores.

### 2. **Implementa√ß√£o do Web Scraping**
Desenvolvemos scripts em Python utilizando `requests` para obter o conte√∫do das p√°ginas e `BeautifulSoup` para parsear o HTML. O c√≥digo foi estruturado para navegar por m√∫ltiplas p√°ginas (at√© 15 p√°ginas) e extrair dados de cada artigo listado.

```python
import requests
from bs4 import BeautifulSoup
import json
import re
import os

# Cabe√ßalho para simular um navegador
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
}

def extrair_info_artigo_lista(article):
    """Extrai informa√ß√µes b√°sicas da lista de artigos."""
    try:
        title = article.find("h2", class_="articleTitle-mtY5p").get_text(strip=True)
        link = article.find("a", class_="articleLink-2OMNo")["href"]
        full_link = "https://www.scientificamerican.com" + link
        kicker = article.find("div", class_="kicker-EEaW-").get_text(strip=True)
        category, date = kicker.split("December")
        date = "December" + date
        summary = article.find("div", class_="dek-KweYs").get_text(strip=True)
        authors = article.find("p", class_="authors-NCGt1").get_text(strip=True)
        return {
            "title": title,
            "link": full_link,
            "category": category.strip(),
            "date": date.strip(),
            "summary": summary,
            "authors": authors
        }
    except:
        return None

# URL base da lista de artigos
base_url = "https://www.scientificamerican.com/latest/"

# N√∫mero de p√°ginas que voc√™ quer coletar
num_pages = 15

# Cria a pasta "data" se ela n√£o existir
output_dir = "data"
os.makedirs(output_dir, exist_ok=True)

post_count = 1  # Inicializa o contador de postagens (fora do loop)

for page in range(1, num_pages + 1):
    # Cria a URL da p√°gina
    url = base_url if page == 1 else f"{base_url}?page={page}"
    print(f"Extraindo dados da p√°gina {page}: {url}")
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.find_all("article", class_="article-pFLe7")
    
    for article in articles:
        info_lista = extrair_info_artigo_lista(article)
        if info_lista:
            # Aqui seria chamada a fun√ß√£o para extrair informa√ß√µes detalhadas
            # e salvar os dados em JSON
            pass
```

### 3. **Extra√ß√£o de Informa√ß√µes Detalhadas**
Al√©m das informa√ß√µes b√°sicas, desenvolvemos fun√ß√µes para acessar cada artigo individualmente e extrair dados adicionais, como tempo de leitura, tipo do artigo, dificuldade, sentimento e o texto completo.

### 4. **Armazenamento dos Dados**
Os dados extra√≠dos s√£o salvos em arquivos JSON na pasta `data`, permitindo f√°cil acesso e manipula√ß√£o futura.

### 5. **An√°lise de Dados**
Utilizamos `pandas` para organizar os dados em DataFrames, facilitando a an√°lise estat√≠stica. Aplicamos visualiza√ß√µes com `seaborn` e `matplotlib` para identificar padr√µes e tend√™ncias. A modelagem de t√≥picos com LDA foi realizada para categorizar os artigos em diferentes temas.

## üß© Desafios Enfrentados

### 1. **Estrutura Vari√°vel das P√°ginas**
Uma das principais dificuldades foi lidar com a inconsist√™ncia na estrutura HTML das p√°ginas e dos artigos individuais. Alguns elementos n√£o estavam sempre presentes ou tinham classes din√¢micas, exigindo implementa√ß√µes com tratamentos de exce√ß√µes.

### 2. **Decodifica√ß√£o de JSON Incorporado**
Muitos artigos incorporavam dados em scripts JSON dentro do HTML. Inicialmente, enfrentamos erros de decodifica√ß√£o devido a caracteres especiais e formata√ß√µes inconsistentes. Foi necess√°rio aplicar express√µes regulares (`re`) para limpar e preparar os dados antes da convers√£o.

```python
import re

def limpar_json(data):
    # Remove quebras de linha e caracteres de controle
    data = re.sub(r'[\n\r\t]+', ' ', data)
    # Substitui aspas
    data = data.replace('\"', '"')
    return data
```

### 3. **Manuten√ß√£o de Sess√µes e Limita√ß√£o de Requisi√ß√µes**
Para evitar bloqueios por parte do servidor, implementamos pausas entre as requisi√ß√µes e utilizamos cabe√ßalhos personalizados para simular um navegador real. Contudo, a frequ√™ncia das requisi√ß√µes ainda representou um desafio para a estabilidade do scraping.

### 4. **Processamento de Texto Extenso**
A extra√ß√£o e limpeza do texto completo dos artigos demandou aten√ß√£o para remover elementos indesejados, como an√∫ncios e links internos, garantindo que apenas o conte√∫do relevante fosse analisado.

## üìä Resultados e An√°lises

### 1. **Distribui√ß√£o de Dificuldade dos Artigos**

![Distribui√ß√£o de Dificuldade dos Artigos](data/1.png)

- **Observa√ß√£o**: A maioria dos artigos est√° marcada como "easy", seguida por "medium" e poucos "hard".
- **Insight**: O conte√∫do tende a ser mais acess√≠vel, possivelmente visando um p√∫blico amplo.

### 2. **Distribui√ß√£o de Sentimento dos Artigos**

![Distribui√ß√£o de Sentimentos dos Artigos](data/2.png)

- **Observa√ß√£o**: A categoria "neutral" predomina, seguida de "positive" e "negative".
- **Insight**: Artigos cient√≠ficos geralmente mant√™m uma perspectiva mais imparcial.

### 3. **Distribui√ß√£o do Tempo de Leitura (minutos)**

![Distribui√ß√£o do Tempo de Leitura (minutos)](data/3.png)  
![Boxplot do Tempo de Leitura (minutos)](data/4.png)

- **Observa√ß√£o**: Grande concentra√ß√£o entre 2 e 6 minutos de leitura, com alguns outliers.
- **Insight**: Conte√∫dos curtos s√£o preferidos para engajamento r√°pido, enquanto textos mais longos atendem a leitores mais dedicados.

### 4. **Tempo de Leitura x Dificuldade**

![Tempo de Leitura x Dificuldade](data/5.png)

- **Observa√ß√£o**: Artigos "easy" t√™m tempo de leitura medianamente mais curto, mas h√° exce√ß√µes.
- **Insight**: A dificuldade n√£o est√° necessariamente ligada ao comprimento do texto, sugerindo clareza na escrita.

### 5. **Distribui√ß√£o do Tamanho do Texto (n√∫mero de palavras)**

![Distribui√ß√£o do Tamanho do Texto](data/6.png)

- **Observa√ß√£o**: A maioria dos artigos tem entre 500 e 2.000 palavras, com alguns muito extensos.
- **Insight**: Textos moderados s√£o predominantes, equilibrando profundidade e acessibilidade.

### 6. **Correla√ß√£o entre Tamanho do Texto e Tempo de Leitura**

![Correla√ß√£o N¬∫ Palavras vs Tempo de Leitura](data/7.png)

- **Observa√ß√£o**: Rela√ß√£o quase linear entre o n√∫mero de palavras e o tempo de leitura.
- **Insight**: Valida√ß√£o da consist√™ncia dos dados, onde textos mais longos demandam mais tempo para leitura.

### 7. **Nuvem de Palavras dos Resumos**

![Nuvem de Palavras - Resumos](data/8.png)

- **Observa√ß√£o**: Termos como "science", "climate", "system", "Earth" s√£o predominantes.
- **Insight**: Os artigos abordam amplamente temas cient√≠ficos e de relev√¢ncia global.

### 8. **Modelagem de T√≥picos (LDA)**

#### T√≥pico 1
```
science like people new health really know just said time virus think public animals papp
```
- **Interpreta√ß√£o**: Foco em **sa√∫de**, **epidemiologia** e impacto da ci√™ncia na sociedade.

#### T√≥pico 2
```
says like people sleep brain just study time research years ai researchers new university human
```
- **Interpreta√ß√£o**: Enfatiza **neuroci√™ncia** e **intelig√™ncia artificial**, explorando comportamento humano e tecnologias emergentes.

#### T√≥pico 3
```
says earth planet scientists years like stars life space new star way care people study
```
- **Interpreta√ß√£o**: Direcionado a **astronomia** e **astrobiologia**, incluindo temas de explora√ß√£o espacial e preserva√ß√£o ambiental.

## üìù Conclus√£o e Insights

1. **N√≠vel de Escrita e Extens√£o**:
   - Predomin√¢ncia de artigos com dificuldade "easy" e tempo de leitura entre 2 e 6 minutos, indicando uma abordagem acess√≠vel e voltada para um p√∫blico amplo.
   - Presen√ßa de artigos mais longos e "hard" sugere diversidade no conte√∫do, atendendo tamb√©m a leitores mais dedicados.

2. **Sentimento**:
   - A maioria dos artigos apresenta um sentimento "neutral", alinhado com a natureza cient√≠fica e informativa da publica√ß√£o.
   - Artigos "positive" e "negative" fornecem perspectivas balanceadas, enriquecendo a an√°lise com diferentes tonalidades.

3. **T√≥picos Dominantes**:
   - **Sa√∫de e Epidemiologia**: Reflete a import√¢ncia de temas relacionados √† sa√∫de p√∫blica e doen√ßas infecciosas.
   - **Neuroci√™ncia e Intelig√™ncia Artificial**: Indica um interesse crescente em tecnologias e suas interse√ß√µes com o comportamento humano.
   - **Astronomia e Preserva√ß√£o Ambiental**: Destaca a relev√¢ncia de quest√µes planet√°rias e explora√ß√£o espacial.

4. **Rela√ß√£o Tamanho x Tempo de Leitura**:
   - A correla√ß√£o quase linear confirma a consist√™ncia dos dados, em que textos mais extensos naturalmente demandam mais tempo de leitura.

5. **Palavras-Chave Frequentes**:
   - Termos como "science", "Earth", "climate", "space" indicam os principais focos tem√°ticos dos artigos, alinhando-se com as tend√™ncias atuais de pesquisa e divulga√ß√£o cient√≠fica.

6. **Desafios Superados**:
   - A adapta√ß√£o √†s estruturas vari√°veis das p√°ginas e a limpeza eficaz dos dados JSON incorporados foram importantes para garantir a integridade e a precis√£o dos dados coletados.
   - A implementa√ß√£o de t√©cnicas de scraping e processamento de texto assegurou uma coleta eficiente, apesar das barreiras t√©cnicas enfrentadas.

7. **Aplica√ß√µes Pr√°ticas**:
   - **Decis√µes Estrat√©gicas**: As an√°lises fornecem uma base s√≥lida para entender quais temas e formatos repercutem mais entre o p√∫blico, auxiliando na cria√ß√£o de conte√∫do futuro.
   - **Melhoria de Conte√∫do**: Identificar a predomin√¢ncia de conte√∫dos "easy" pode orientar a diversifica√ß√£o de n√≠veis de dificuldade para atender a diferentes segmentos de leitores.
   - **Engajamento de Leitores**: Compreender a distribui√ß√£o de tempo de leitura e tamanho dos textos ajuda a otimizar o engajamento, balanceando entre conte√∫dos r√°pidos e aprofundados.


